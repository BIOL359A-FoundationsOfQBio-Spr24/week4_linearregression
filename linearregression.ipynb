{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A | Linear Regression\n",
    "### Spring 2024, Week 4\n",
    "Objectives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, Dropdown, Checkbox, HBox, VBox, Button, Output, Layout, IntSlider\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r week4_linearregression/\n",
    "! git clone https://github.com/BIOL359A-FoundationsOfQBio-Spr24/week4_linearregression.git\n",
    "! cp -r week4_linearregression/* .\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression\n",
    "\n",
    "Linear regression assumes a linear relationship between the input variables (predictors) and the single output variable (response). When the input variable is one, the method is referred to as simple linear regression. When there are multiple input variables, it is called multiple linear regression.\n",
    "\n",
    "## Key Terms in Linear Regression\n",
    "\n",
    "- **Predictor Variables (Independent variables)**: These are the variables/input that predict or affect the outcome. They are used as the basis to predict the response variable.\n",
    "- **Response Variable (Dependent variable)**: This is the outcome variable that the model is trying to predict, using predictor variables.\n",
    "- **Coefficients**: These are the weights assigned to the predictor variables. They signify the change in the response variable for a one-unit change in the predictor.\n",
    "- **Intercept**: This is the expected mean value of the response variable when all the predictors are equal to zero.\n",
    "- **Residual Error**: This is the difference between the observed values and the values predicted by the model. It represents the error in the predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables:\n",
    "1. The variable x is the predictor, or independent variable.\n",
    "2. The variable y is the response, or dependent variable.\n",
    "\n",
    "Simple linear regression uses one independent variable to explain or predict the outcome of the dependent variable Y, based on the linear relationship between X and Y.\n",
    "\n",
    "### Coefficient of Determination\n",
    "\n",
    "The __Coefficient of Determination__, or $R^2$, conceptually represents the \"amount of variance in y is explained by x\" and can be thought of as how well the model fits the training data. This is a seperate concept to the Pearson Correlation Coefficient, $\\rho$, which in the cases of a __strictly linear regression__, (regression which is linear with respect to both coefficients and parameters) $R^2 = \\rho^2$. $R^2$ is calculated as follows:\n",
    "\n",
    "$$ R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}$$\n",
    "\n",
    "Where \n",
    "- SSE = error of sum of squares\n",
    "- SSR = regression sum of squares\n",
    "- SST = total sum of squares\n",
    "\n",
    "You can also calculate the adjusted $R^2$ which accounts for the model size and parameters. This way, you can compare the adjusted $R^2$ of simple models to the adjusted $R^2$ of complex models, which you can't necessarily do if you just use  $R^2$. \n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{\\frac{SSE}{n-k-1}}{\\frac{SST}{n - 1}}$$\n",
    "\n",
    "Where \n",
    "- n = number of data points\n",
    "- k number of independent variables\n",
    "\n",
    "Generally speaking, $R^2$ is used to characterize the fit of a model, where as $\\rho$ is used to characterize the relationship between two variables. \n",
    "\n",
    "We will perform a simple linear regression between the response variable, y, and the feature, x. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNMENT QUESTIONS:\n",
    "- Please answer questions 6 and 7 in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to the Allen Institute for Cell Science Cell Feature Dataset\n",
    "\n",
    "The [Allen Institute for Cell Science Cell Feature Dataset](https://www.allencell.org/data-downloading.html#DownloadFeatureData) was published with [Viana et al. 2023](https://www.nature.com/articles/s41586-022-05563-7). These data are visualized in the [Cell Feature Explorer](https://cfe.allencell.org/).\n",
    "\n",
    "**Dataset Features**:\n",
    "- **`CellId`**: A unique identifier for each cell in the dataset.\n",
    "- **`Nuclear Volume [fL]`**: The volume of the cell's nucleus measured in femtoliters (fL).\n",
    "- **`Cellular Volume [fL]`**: The total volume of the cell measured in femtoliters (fL).\n",
    "- **`Nuclear Surface Area [um^2]`**: The surface area of the cell’s nucleus measured in square micrometers (um^2).\n",
    "- **`Cellular Surface Area [um^2]`**: The total surface area of the cell measured in square micrometers (um^2).\n",
    "- **`Radial Proximity [unitless]`**: A unitless measure that quantifies the cell's position relative to the center of the organ or tissue.\n",
    "- **`Apical Proximity [unitless]`**: Another unitless measure that describes how close the cell is to the apical surface of the tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('data/aics_cfe_cell-features_v1.8.csv')\n",
    "\n",
    "# Print the shape of the data (rows, columns)\n",
    "print(data.shape)\n",
    "\n",
    "# Display the first few rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNMENT QUESTION:\n",
    "- Please complete question 8 in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interactive visualization below allows you to select two different features from the Allen dataset and plot them against each other in a scatter plot with the simple linear regression line.\n",
    "\n",
    "The regression outputs are:\n",
    "- **beta1**: This is the slope of the regression line, representing the change in the dependent variable (y) for each unit change in the explanatory variable (x).\n",
    "- **beta0**: This is the intercept of the regression line, representing the value of y when x is 0.\n",
    "- **R² (R-squared)**: This statistic measures the proportion of variance in the dependent variable that is predictable from the explanatory variable.\n",
    "- **Pearson's Correlation Coefficient**: This measures the linear correlation between two variables, ranging from -1 to 1, where 1 means a perfect positive correlation, -1 means a perfect negative correlation, and 0 means no linear correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(x_feature, y_feature):\n",
    "    # Filtering out the CellID and setting the data for regression\n",
    "    x = data[x_feature].values.reshape(-1, 1)\n",
    "    y = data[y_feature].values.reshape(-1, 1)\n",
    "    \n",
    "    # Fitting the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    # Calculating regression statistics\n",
    "    beta1 = model.coef_[0][0]\n",
    "    beta0 = model.intercept_[0]\n",
    "    r_squared = model.score(x, y)\n",
    "    corr_coef, _ = pearsonr(data[x_feature], data[y_feature])\n",
    "    \n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x, y, color='blue', label='Data points')\n",
    "    plt.plot(x, y_pred, color='red', label='Regression line')\n",
    "    plt.xlabel(x_feature)\n",
    "    plt.ylabel(y_feature)\n",
    "    plt.title(f'Scatter plot of {x_feature} vs {y_feature}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Displaying the regression results\n",
    "    print(f\"beta1 (Slope): {beta1:.4f}\")\n",
    "    print(f\"beta0 (Intercept): {beta0:.4f}\")\n",
    "    print(f\"R-squared: {r_squared:.4f}\")\n",
    "    print(f\"Pearson's Correlation Coefficient: {corr_coef:.4f}\")\n",
    "\n",
    "# Creating dropdowns for feature selection\n",
    "features = [col for col in data.columns if col != 'CellId']  # Exclude 'CellID'\n",
    "_ = interact(plot_regression, x_feature=Dropdown(options=features), y_feature=Dropdown(options=features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCUSSION QUESTION:\n",
    "- What is the relationship between R^2 and Pearson's Correlation Coefficient in the above plot? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical tests and $\\beta$ coefficients\n",
    "\n",
    "When performing linear regression, each regression coefficient ($\\beta$) represents the expected change in the dependent variable for a one-unit change in the corresponding independent variable, holding all other variables constant. To understand the statistical significance of these coefficients, you can conduct hypothesis tests.\n",
    "\n",
    "The null hypothesis typically states that the coefficient in question, ($\\beta_i$)​, has no effect on the dependent variable, implying $\\beta_i=0$. To test this hypothesis, you use the t-statistic, which is calculated by dividing the estimated coefficient by its standard error. The resulting t-value is then compared against a critical value from the t-distribution, considering the desired level of confidence and the degrees of freedom in your data.\n",
    "\n",
    "When the independent variable in the regression is binary or boolean (i.e., it only takes two values), the regression model simplifies the comparison between two groups. In this scenario, testing $\\beta_i=0$ is analogous to performing an independent samples t-test between these two groups. The t-test compares the means of two groups and tests whether they are statistically different from each other. In the regression context, the coefficient $\\beta_i$​ for a binary variable measures the difference in the mean of the dependent variable between the two groups defined by the binary variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solidify this concept, we will use the Wisconsin Diagnostic Breast Cancer dataset once again and use the boolean dianosis status of each cell to try to predict the other cell features. A reminder on the dataset:\n",
    "```\n",
    "\tFeatures are computed from a digitized image of a fine needle\n",
    "\taspirate (FNA) of a breast mass.  They describe\n",
    "\tcharacteristics of the cell nuclei present in the image.\n",
    "\tA few of the images can be found at\n",
    "\thttp://www.cs.wisc.edu/~street/images/\n",
    "\n",
    "\tSeparating plane described above was obtained using\n",
    "\tMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "\tConstruction Via Linear Programming.\" Proceedings of the 4th\n",
    "\tMidwest Artificial Intelligence and Cognitive Science Society,\n",
    "\tpp. 97-101, 1992], a classification method which uses linear\n",
    "\tprogramming to construct a decision tree.  Relevant features\n",
    "\twere selected using an exhaustive search in the space of 1-4\n",
    "\tfeatures and 1-3 separating planes.\n",
    "\n",
    "\tThe actual linear program used to obtain the separating plane\n",
    "\tin the 3-dimensional space is that described in:\n",
    "\t[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "\tProgramming Discrimination of Two Linearly Inseparable Sets\",\n",
    "\tOptimization Methods and Software 1, 1992, 23-34].\n",
    "    \n",
    "    Source:\n",
    "    W.N. Street, W.H. Wolberg and O.L. Mangasarian \n",
    "\tNuclear feature extraction for breast tumor diagnosis.\n",
    "\tIS&T/SPIE 1993 International Symposium on Electronic Imaging: Science\n",
    "\tand Technology, volume 1905, pages 861-870, San Jose, CA, 1993.\n",
    "```\n",
    "\n",
    "What do all the column names mean?\n",
    "\n",
    "- ID number\n",
    "- Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry \n",
    "- fractal dimension (\"coastline approximation\" - 1) - a measure of \"complexity\" of a 2D image.\n",
    "\n",
    "\n",
    "Cateogory Distribution: 357 benign, 212 malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean_data\n",
    "\n",
    "original_cancer_dataset = clean_data.generate_clean_dataframe()\n",
    "cancer_dataset = original_cancer_dataset.reset_index()\n",
    "cancer_dataset.drop(\"ID\", axis=1, inplace=True)\n",
    "cancer_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visuzalisation below allows you to select a feature and see the distribution of that feature for malignant vs benign cells. It then plots diagnosis status as the independent variable and the selected feature as the dependent variable and performs simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr, t\n",
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "\n",
    "# Mapping the 'diagnosis' to a binary variable for regression purposes\n",
    "cancer_dataset['diagnosis_code'] = cancer_dataset['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "def interactive_feature_visualization(feature):\n",
    "    # Plotting the distribution of the feature for malignant and benign tumors\n",
    "    malignant = cancer_dataset[cancer_dataset['diagnosis'] == 'M'][feature]\n",
    "    benign = cancer_dataset[cancer_dataset['diagnosis'] == 'B'][feature]\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(malignant, color='red', kde=True, stat=\"density\", linewidth=0, label='Malignant')\n",
    "    sns.histplot(benign, color='blue', kde=True, stat=\"density\", linewidth=0, label='Benign')\n",
    "    plt.title(f'Distribution of {feature} for Malignant vs Benign')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(x='diagnosis', y=feature, data=cancer_dataset)\n",
    "    plt.title(f'Box Plot of {feature} by Diagnosis')\n",
    "\n",
    "    # Regression Analysis\n",
    "    x = cancer_dataset[['diagnosis_code']].values  # Independent variable\n",
    "    y = cancer_dataset[feature].values  # Dependent variable\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    # Scatter plot with regression line\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(x, y, color='green', label='Actual data')\n",
    "    plt.plot(x, y_pred, color='black', linewidth=2, label='Regression line')\n",
    "    plt.title(f'Scatter Plot and Regression Line for {feature}')\n",
    "    plt.xlabel('Diagnosis Code (0=Benign, 1=Malignant)')\n",
    "    plt.ylabel(feature)\n",
    "    plt.xticks([0, 1], ['Benign', 'Malignant'])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Calculating regression statistics\n",
    "    beta1 = model.coef_[0]\n",
    "    beta0 = model.intercept_\n",
    "    r_squared = model.score(x, y)\n",
    "    n = len(y)\n",
    "    residual_std_error = np.sqrt(np.sum((y - y_pred) ** 2) / (n - 2))\n",
    "    corr_coef, _ = pearsonr(cancer_dataset['diagnosis_code'], y)\n",
    "    std_error_beta1 = residual_std_error / np.sqrt(np.sum((x - np.mean(x)) ** 2))\n",
    "    t_statistic = beta1 / std_error_beta1\n",
    "    p_value = (1 - t.cdf(np.abs(t_statistic), df=n-2)) * 2  # two-tailed p-value\n",
    "\n",
    "    print(f\"Linear Regression Analysis for {feature} based on Diagnosis:\")\n",
    "    print(f\"beta1 (Effect of Malignant over Benign): {beta1:.4f}\")\n",
    "    print(f\"beta0 (Intercept, Effect of Benign): {beta0:.4f}\")\n",
    "    print(f\"R-squared: {r_squared:.4f}\")\n",
    "    print(f\"Pearson's Correlation Coefficient: {corr_coef:.4f}\")\n",
    "    print(f\"t-statistic for beta1: {t_statistic:.4f}\")\n",
    "    print(f\"p-value for beta1: {p_value:.4f}\")\n",
    "\n",
    "# Creating dropdown for feature selection, excluding 'diagnosis'\n",
    "features = [col for col in cancer_dataset.columns if col not in ['diagnosis', 'diagnosis_code']]\n",
    "_ = interact(interactive_feature_visualization, feature=Dropdown(options=features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCUSSION QUESTION:\n",
    "- How does the R-squared value relate to the Pearson's Correlation Coefficient? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "We have other variables that we can use to explore the relationships in this dataset. We can use multiple linear regression to include more variables, where we are drawing a multi-dimensional shape, rather than simply a line, as our model. \n",
    "\n",
    "The visualization below allows you to perform multiple linear regression. Select a dependent (or response variable) using the drop down and select independent variables using the check boxes. The plot shown is a parity plot, which shows the true value of the specified dependent variable plotted against the model-predicted value of the specified dependent variable. Selecting one independent variable will perform simple linear regression, but selecting more independent variables will perform multiple linear regression. The dotted black line is the parity line. It depicts where all the data would be if the model perfectly predicted the values for the dependent variable. Try adding multiple independent variables into the regression by checking the boxes and see how it affects the model's ability to predict different dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset['diagnosis_code'] = cancer_dataset['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Define plot_output outside the function to ensure it's in the correct scope\n",
    "plot_output = Output()\n",
    "\n",
    "def multiple_regression_visualization(response):\n",
    "    features = [col for col in cancer_dataset.columns if col not in [response, 'diagnosis', 'diagnosis_code']]\n",
    "    checkboxes = {feature: Checkbox(value=False, description=feature, style={'description_width': 'initial'}) for feature in features}\n",
    "    checkbox_container = VBox(list(checkboxes.values()), layout=Layout(width='100%', overflow='auto'))\n",
    "    button = Button(description=\"Update Regression\")\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        selected_features = [feat for feat, cb in checkboxes.items() if cb.value]\n",
    "        perform_regression(response, selected_features)\n",
    "    \n",
    "    button.on_click(on_button_clicked)\n",
    "    \n",
    "    # Only display widgets and plot_output container together\n",
    "    display(VBox([checkbox_container, button, plot_output]))\n",
    "\n",
    "def perform_regression(response, selected_features):\n",
    "    with plot_output:\n",
    "        plot_output.clear_output(wait=True)\n",
    "        if selected_features:\n",
    "            X = cancer_dataset[selected_features]\n",
    "            y = cancer_dataset[response]\n",
    "            \n",
    "            # Fitting multiple linear regression\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            y_pred = model.predict(X)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            # Parity plot\n",
    "            scatter = plt.scatter(y, y_pred, c=cancer_dataset['diagnosis_code'], cmap=plt.cm.coolwarm, alpha=0.6, edgecolors='w')\n",
    "            plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)  # Line of perfect parity\n",
    "            plt.xlabel('True Values')\n",
    "            plt.ylabel('Predictions')\n",
    "            plt.title('Parity Plot of Predictions vs. True Values')\n",
    "            \n",
    "            # Creating a custom legend for diagnosis\n",
    "            legend1 = plt.legend(*scatter.legend_elements(), title=\"Diagnosis\")\n",
    "            legend1.get_texts()[0].set_text('Benign')\n",
    "            legend1.get_texts()[1].set_text('Malignant')\n",
    "            plt.gca().add_artist(legend1)\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            # Output the regression coefficients and R-squared\n",
    "            print(\"Regression coefficients:\")\n",
    "            print(\"Intercept:\", model.intercept_)\n",
    "            for feature, coef in zip(selected_features, model.coef_):\n",
    "                print(f\"{feature}: {coef}\")\n",
    "            print(\"R-squared:\", r2_score(y, y_pred))\n",
    "        else:\n",
    "            print(\"Please select at least one feature.\")\n",
    "\n",
    "# Creating dropdown for response variable selection\n",
    "response_features = [col for col in cancer_dataset.columns if col not in ['diagnosis', 'diagnosis_code']]\n",
    "_ = interact(multiple_regression_visualization, response=Dropdown(options=response_features, description=\"Select Response Variable:\", style={'description_width': 'initial'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNMENT QUESTION:\n",
    "- Please answer questions 9 and 10 in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCUSSION QUESTION:\n",
    "- Can you compare the significance of impact of each independent variable on the dependent variable using the magnitude of their respective regression coefficients? (Precursor to the next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "We need to be careful about how we interpret parameters in relation to each other. One method that we can use to compare coefficients is to __standardize__ the features. This will effectively shift the distribution of all of our features to have a Standard Normal ($\\mu=0,\\sigma^2=1$) distribution (a normal distribution with a mean of 0 and a variance of 1). We can then compare the coefficients to each other, and interpret the magnitude within the model. \n",
    "\n",
    "Note: This transformation prevents you from making a direct interpretation of the coefficient as it relates to the actual value, and is now unitless. You should interpret these as being \"model units\". This transformation also means that the y-intercept will be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset['diagnosis_code'] = cancer_dataset['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "plot_output = Output()\n",
    "\n",
    "def multiple_regression_standardized_visualization(response):\n",
    "    features = [col for col in cancer_dataset.columns if col not in [response, 'diagnosis', 'diagnosis_code']]\n",
    "    checkboxes = {feature: Checkbox(value=False, description=feature, style={'description_width': 'initial'}) for feature in features}\n",
    "    checkbox_container = VBox(list(checkboxes.values()), layout=Layout(width='100%', overflow='auto'))\n",
    "    button = Button(description=\"Update Regression\")\n",
    "\n",
    "    def on_button_clicked(b):\n",
    "        selected_features = [feat for feat, cb in checkboxes.items() if cb.value]\n",
    "        perform_standardized_regression(response, selected_features)\n",
    "\n",
    "    button.on_click(on_button_clicked)\n",
    "    \n",
    "    display(VBox([checkbox_container, button, plot_output]))\n",
    "\n",
    "def perform_standardized_regression(response, selected_features):\n",
    "    with plot_output:\n",
    "        plot_output.clear_output(wait=True)\n",
    "        if selected_features:\n",
    "            X = cancer_dataset[selected_features]\n",
    "            y = cancer_dataset[response]\n",
    "            \n",
    "            # Standardizing both the features and the response\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "            X_scaled = scaler_X.fit_transform(X)\n",
    "            y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Fitting multiple linear regression\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_scaled, y_scaled)\n",
    "            \n",
    "            print(\"Regression coefficients for fully standardized data:\")\n",
    "            print(f\"Intercept (should be close to 0) ~ {model.intercept_:.3}\")\n",
    "            for feature, coef in zip(selected_features, model.coef_):\n",
    "                print(f\"{feature} ~ {coef:.3}\")\n",
    "            print(f\"R-squared: {model.score(X_scaled, y_scaled):.4f}\")\n",
    "        else:\n",
    "            print(\"Please select at least one feature.\")\n",
    "\n",
    "response_features = [col for col in cancer_dataset.columns if col not in ['diagnosis', 'diagnosis_code']]\n",
    "_ = interact(multiple_regression_standardized_visualization, response=Dropdown(options=response_features, description=\"Select Response Variable:\", style={'description_width': 'initial'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNMENT QUESTION:\n",
    "- Please answer question 11 in the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Feature Engineering\n",
    "\n",
    "Feature engineering is the process of using domain knowledge to select, modify, or create new features from raw data to increase the predictive power of machine learning algorithms. In the context of regression, this often involves transforming or combining input data to better expose the underlying patterns to the learning algorithm.\n",
    "\n",
    "#### Polynomial Features in Regression\n",
    "\n",
    "We are going to generate data from a quadratic equation with some noise, and then we are going to try and fit a new linear model:\n",
    "\n",
    "$$ y = \\beta_0 + \\sum_{i=1}^{degrees} \\beta_i x^i$$ \n",
    "\n",
    "Where we are going to take an original dataset of 1 feature, and generate multiple new features from the original feature. From this equation, if we set `degrees` equal to 1, we are doing a simple linear regression. \n",
    "\n",
    "If we set `degrees` equal to 2, we are fitting a MLR model with $x$ and $x^2$ as our features. \n",
    "Creating polynomial features is one way that we can do feature engineering.\n",
    "\n",
    "When creating polynomial features for a regression model, the primary purpose is to add complexity to the model, allowing it to capture more intricate patterns in the data. Polynomial features enable the model to fit non-linear relationships within a linear model framework by considering higher-degree terms of the input features. This can be crucial when the relationship between the independent and dependent variables is not linear but still follows some predictable pattern that higher-degree polynomials can approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from ipywidgets import interact, IntSlider, Layout\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_data(num_points):\n",
    "    np.random.seed(0)\n",
    "    X = 2 - 3 * np.random.normal(0, 1, num_points)\n",
    "    y = X - 2 * (X ** 2) + np.random.normal(-3, 3, num_points)\n",
    "    return X, y\n",
    "\n",
    "def plot_polynomial_regression(num_points=20, degree=2):\n",
    "    X, y = generate_data(num_points)\n",
    "    X = X[:, np.newaxis]\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=degree)\n",
    "    X_poly = polynomial_features.fit_transform(X)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    y_pred = model.predict(X_poly)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, y, s=30, marker='o', label=\"Data Points\")\n",
    "    sorted_zip = sorted(zip(X, y_pred))\n",
    "    X, y_pred = zip(*sorted_zip)\n",
    "    plt.plot(X, y_pred, color='r', label=f\"Polynomial Degree {degree}\")\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Polynomial Regression Fit')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"R-squared for polynomial degree {degree}: {r2_score(y, y_pred):.4f}\")\n",
    "\n",
    "# Interactive widgets with adjusted layout\n",
    "_ = interact(plot_polynomial_regression, \n",
    "         num_points=IntSlider(value=70, min=10, max=100, step=5, description='Number of Data Points', style={'description_width': 'initial'}, layout=Layout(width='500px')),\n",
    "         degree=IntSlider(value=1, min=1, max=10, step=1, description='Polynomial Degree', style={'description_width': 'initial'}, layout=Layout(width='500px')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNMENT QUESTION:\n",
    "- Please complete assignment questions 12 and 13\n",
    "\n",
    "DISCUSSION QUESTION:\n",
    "- What degree polynomial is sufficient to fit a linear model to these data? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
